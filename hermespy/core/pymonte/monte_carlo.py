# -*- coding: utf-8 -*-

from __future__ import annotations
from collections.abc import Sequence
from contextlib import nullcontext
from logging import INFO, ERROR
from os import path
from time import perf_counter, sleep
from typing import Generic, Type

import numpy as np
import ray as ray
from rich.console import Console, Group
from rich.live import Live
from rich.progress import Progress, SpinnerColumn, TimeElapsedColumn
from rich.table import Table
from scipy.io import savemat

from ..definitions import ConsoleMode
from ..visualize import PlotVisualization
from .actors import MonteCarloActor, MonteCarloCollector, MonteCarloQueueManager
from .definitions import MO
from .evaluation import Evaluator, EvaluationResult
from .grid import GridDimension

__author__ = "Jan Adler"
__copyright__ = "Copyright 2025, Barkhausen Institut gGmbH"
__credits__ = ["Jan Adler"]
__license__ = "AGPLv3"
__version__ = "1.5.0"
__maintainer__ = "Jan Adler"
__email__ = "jan.adler@barkhauseninstitut.org"
__status__ = "Prototype"


class MonteCarloResult(object):
    """Result of a Monte Carlo simulation."""

    __grid: Sequence[GridDimension]
    __evaluators: Sequence[Evaluator]
    __performance_time: float  # Absolute time required to compute the simulation
    __results: Sequence[EvaluationResult]

    def __init__(
        self,
        grid: Sequence[GridDimension],
        evaluators: Sequence[Evaluator],
        results: Sequence[EvaluationResult],
        performance_time: float,
    ) -> None:
        """
        Args:
            grid: Dimensions over which the simulation has swept.
            evaluators: Evaluators used to evaluated the simulation artifacts.
            results: Evaluation results generated by the evaluators.
            performance_time: Time required to compute the simulation.

        Raises:
            ValueError: If the dimensions of `samples` do not match the supplied sweeping dimensions and evaluators.
        """

        # Store init arguments as attributes
        self.__grid = grid
        self.__evaluators = evaluators
        self.__results = results
        self.__performance_time = performance_time

    @property
    def performance_time(self) -> float:
        """Simulation runtime in seconds."""

        return self.__performance_time

    def plot(self) -> list[PlotVisualization]:
        """Plot evaluation figures for all contained evaluator artifacts.

        Returns: Container of all generated plots.
        """
        return [result.visualize() for result in self.__results]

    def save_to_matlab(self, file: str) -> None:
        """Save simulation results to a matlab file.

        Args:
            file: File location to which the results should be saved.
        """

        mat_dict = {
            "dimensions": [d.title for d in self.__grid],
            "evaluators": [evaluator.abbreviation for evaluator in self.__evaluators],
            "performance_time": self.__performance_time,
        }

        # Append evaluation array representions
        for r, result in enumerate(self.__results):
            mat_dict[f"result_{r}"] = result.to_array()

        # Append evaluation array representions
        for r, result in enumerate(self.__results):
            mat_dict[f"result_{r}"] = result.to_array()

        for dimension in self.__grid:
            mat_dict[dimension.title] = dimension.sample_points

        # Save results in matlab file
        savemat(file, mat_dict)

    def print(self, console: Console | None = None) -> None:
        """Print a text representation of the simulation result.

        Args:
            console:
                Rich console to print to.
                If not provided, a new one will be initialized.
        """

        _console = Console() if console is None else console

        # Render results into a rich table
        table = Table()

        # Generate columns
        for dimension in self.__grid:
            table.add_column(dimension.title, style="cyan")
        for evaluator in self.__evaluators:
            table.add_column(evaluator.abbreviation)

        # Generate a row for each sweep grid point
        for grid_coordinates in np.ndindex(*[d.num_sample_points for d in self.__grid]):

            # The initial row values are occupied by the grid values for the respective coordinates
            row_values = [g.sample_points[s].title for s, g in zip(grid_coordinates, self.__grid)]

            # The final row values are occupied by the evaluation results
            for result in self.evaluation_results:
                row_values.append(result.to_str(grid_coordinates))

            # Append the row to the table
            table.add_row(*row_values)

        _console.print(table)

    @property
    def evaluation_results(self) -> Sequence[EvaluationResult]:
        """Access individual evaluation results."""

        return self.__results


class MonteCarlo(Generic[MO]):
    """Grid of parameters over which to iterate the simulation."""

    __progress_log_interval: float
    __num_samples: int
    __num_actors: int
    __investigated_object: MO
    __dimensions: list[GridDimension]
    __evaluators: list[Evaluator]
    __console: Console
    __console_mode: ConsoleMode
    __cpus_per_actor: int
    runtime_env: bool
    catch_exceptions: bool

    def __init__(
        self,
        investigated_object: MO,
        num_samples: int,
        evaluators: Sequence[Evaluator] | None = None,
        batch_size: int | None = None,
        num_actors: int | None = None,
        console: Console | None = None,
        console_mode: ConsoleMode = ConsoleMode.INTERACTIVE,
        ray_address: str | None = None,
        ray_port: int = 6379,
        cpus_per_actor: int = 1,
        runtime_env: bool = False,
        catch_exceptions: bool = True,
        progress_log_interval: float = 4.0,
        debug: bool = False,
    ) -> None:
        """
        Args:
            investigated_object:
                Object to be investigated during the simulation runtime.

            num_samples:
                Number of generated samples per grid element.

            evaluators:
                Evaluators used to process the investigated object sample state.

            batch_size:
                Number of samples collected by inidividual actors.
                Refer to :attr:`.batch_size` for more information.

            num_actors:
                Number of dedicated actors spawned during simulation.
                By default, the number of actors will be the number of available CPU cores.

            console:
                Console the simulation writes to.

            console_mode:
                The printing behaviour of the simulation during runtime.

            batch_size:
                Number of samples per section block.
                By default, the size of the simulation grid is selected.

            ray_address:
                The address of the ray head node.
                If None is provided, the head node will be launched in this machine.

            cpus_per_actor:
                Number of CPU cores reserved per actor.
                One by default.

            runtime_env:
                Create a virtual environment on each host.
                Disabled by default.

            catch_exceptions:
                Catch exceptions occuring during simulation runtime.
                Enabled by default.

            progress_log_interval:
                Interval between result logs in seconds.
                1 second by default.

            debug:
                Enables debug mode during simulation runtime.
                Debug mode will add performance-related information to the output and enable the ray dashboard.
        """

        self.runtime_env = runtime_env

        self.__dimensions = []
        self.__investigated_object = investigated_object
        self.__evaluators = [] if evaluators is None else list(evaluators)
        self.num_samples = num_samples
        self.__console = Console(log_path=False) if console is None else console
        self.__console_mode = console_mode
        self.cpus_per_actor = cpus_per_actor
        self.batch_size = batch_size
        self.num_actors = num_actors
        self.catch_exceptions = catch_exceptions
        self.__progress_log_interval = progress_log_interval
        self.__debug = debug

        # Launch ray if not already running
        if not ray.is_initialized():
            ray_context = ray.init(
                address=f"{ray_address}:{ray_port}" if ray_address is not None else None,
                runtime_env=(
                    {"py_modules": self._py_modules(), "pip": self._pip_packages()}
                    if self.runtime_env
                    else None
                ),
                logging_level=INFO if debug else ERROR,
                include_dashboard=debug,
            )

            if debug:
                self.console.log(f"Ray dashboard available at {ray_context.dashboard_url}")

    def simulate(
        self,
        actor: Type[MonteCarloActor],
        additional_dimensions: set[GridDimension] | None = None,
        stage_arguments: dict[str, object] | None = None,
    ) -> MonteCarloResult:
        """Launch the Monte Carlo simulation.

        Args:

            actor:
                The actor from which to generate the simulation samples.

            additional_dimensions:
                Additional dimensions to be added to the simulation grid.

            stage_arguments:
                Arguments to be passed to the simulation stages.
                If the argument is a sequence, the respective stage will iterate over the sequence.

        Returns: A `MonteCarloResult` dataclass containing the simulation results.
        """

        # Generate start timestamp
        start_time = perf_counter()

        # Print meta-information and greeting
        if self.__console_mode != ConsoleMode.SILENT:
            self.console.log(
                f"Launched simulation campaign with {self.num_actors} dedicated actors"
            )

        # Add additional dimensions to configured simulation grid
        _dimensions = self.__dimensions.copy()
        if additional_dimensions is not None:
            _dimensions.extend(additional_dimensions)

        # Sort dimensions after impact in descending order
        def sort(dimension: GridDimension) -> int:
            if dimension.first_impact not in actor.stage_identifiers():
                return 0

            return actor.stage_identifiers().index(dimension.first_impact)

        _dimensions.sort(key=sort)

        num_sections = 1
        min_batch_size = 0
        dimension_str = f"{self.num_samples}"
        for dimension in _dimensions:
            min_batch_size += dimension.num_sample_points
            num_sections *= dimension.num_sample_points
            dimension_str += f" x {dimension.num_sample_points}"
        max_num_samples = num_sections * self.num_samples

        _batch_size = max(min_batch_size, self.batch_size)

        if self.__console_mode != ConsoleMode.SILENT:
            self.console.log(
                f"Generating a maximum of {max_num_samples} = "
                + dimension_str
                + f" samples inspected by {len(self.__evaluators)} evaluators\n"
                + f"Collecting batches of {_batch_size} samples\n"
            )

        # Render simulation grid table
        if self.__console_mode != ConsoleMode.SILENT and len(_dimensions) > 0:
            dimension_table = Table(title="Simulation Grid", title_justify="left")
            dimension_table.add_column("Dimension")
            dimension_table.add_column("Sections", style="cyan")

            for dimension in _dimensions:
                section_str = ""
                for sample_point in dimension.sample_points:
                    section_str += sample_point.title + " "

                # Truncate the section string to the console width
                if (
                    (len(section_str) > self.console.width - 20) and dimension.num_sample_points > 3
                ) or dimension.num_sample_points >= 10:
                    section_str = f"{dimension.sample_points[0].title} {dimension.sample_points[1].title} ... {dimension.sample_points[-1].title}"

                dimension_table.add_row(dimension.title, section_str)

            self.console.print(dimension_table)
            self.console.print()

        # Launch actors, actor manager and
        with self.console.status("Launching Actors ...", spinner="dots") if self.__console_mode == ConsoleMode.INTERACTIVE else nullcontext():  # type: ignore

            # Initialize queue manager
            remote_queue_manager_class = ray.remote(MonteCarloQueueManager)
            queue_manager = remote_queue_manager_class.options(
                num_cpus=0, enable_task_events=self.__debug
            ).remote(self.dimensions, self.num_samples)

            # Initialize actor pool
            remote_actor_class = ray.remote(actor)
            actors: list[MonteCarloActor] = [remote_actor_class.options(num_cpus=self.cpus_per_actor, max_concurrency=2, enable_task_events=self.__debug).remote(queue_manager, (self.__investigated_object, _dimensions, self.__evaluators), a, stage_arguments, self.catch_exceptions) for a in range(self.num_actors)]  # type: ignore[attr-defined]

            # Initialize collector
            remote_collector_class = ray.remote(MonteCarloCollector)
            collector = remote_collector_class.options(
                num_cpus=0, max_concurrency=2, enable_task_events=self.__debug
            ).remote(queue_manager, actors, self.dimensions, self.evaluators)

        # Initialize progress bar
        progress = Progress(
            SpinnerColumn(), *Progress.get_default_columns(), TimeElapsedColumn(), transient=True
        )
        task1 = progress.add_task("Computing", total=1.0)

        # Display results in a live table
        status_group = Group("", progress)

        with Live(status_group, console=self.console) if self.__console_mode == ConsoleMode.INTERACTIVE else nullcontext():  # type: ignore

            # Start running the actors
            for a in actors:
                _ = a.run.remote()  # type: ignore

            # Start the collector
            _ = collector.run.remote()  # type: ignore

            # Compute parameters for displaying interemediate results
            page_counter = 0
            max_num_entries_per_page = 8
            num_pages = num_sections // max_num_entries_per_page + 1
            entries_per_page = num_sections // num_pages + 1

            # Keep executing until all samples are computed
            queue_progress: float = 0.0
            while queue_progress < 1.0:

                page_index = page_counter % num_pages
                page_counter += 1

                sleep(self.__progress_log_interval)

                # Fetch a progress estimate from the queue manager
                queue_progress, active_map = ray.get(queue_manager.query_progress.remote())  # type: ignore

                # Update progress bar visualization
                if self.__console_mode == ConsoleMode.INTERACTIVE:

                    progress.update(task1, completed=queue_progress)

                    if collector is not None:

                        # Fetch an intermediate parameter estimate from the collector
                        intermediate_estimates: list[None | np.ndarray] = ray.get(collector.query_estimates.remote())  # type: ignore

                        results_table = Table(min_width=self.console.measure(progress).minimum)

                        # Add columns for parameter dimensions
                        for dimension in _dimensions:
                            results_table.add_column(dimension.title, style="cyan")

                        # Add columns for available intermediate estimates
                        for evaluator, estimate in zip(self.__evaluators, intermediate_estimates):
                            if estimate is not None:
                                results_table.add_column(evaluator.abbreviation)

                        # Add rows for each parameter estimate
                        for s, section_index in enumerate(
                            np.ndindex(*[d.num_sample_points for d in _dimensions])
                        ):
                            # Skip rows not belonging to the current page
                            if (
                                s < page_index * entries_per_page
                                or s >= (page_index + 1) * entries_per_page
                            ):
                                continue

                            results_row: list[str] = []

                            section_active: bool = active_map[section_index]

                            # Add dimension sample points
                            for dimension, section_idx in zip(_dimensions, section_index):
                                results_row.append(dimension.sample_points[section_idx].title)

                            # Add intermediate estimates
                            for estimate in intermediate_estimates:
                                if estimate is not None:
                                    color_tag = "[orange3]" if section_active else "[green]"
                                    results_row.append(f"{color_tag}{estimate[section_index]:.2f}")

                            results_table.add_row(*results_row)

                        # Add additional empty rows to fill the page
                        # This keeps the table layout consistent
                        for _ in range(entries_per_page - results_table.row_count):
                            results_table.add_row(*[""] * len(results_table.columns))

                        status_group.renderables[0] = results_table

                elif self.__console_mode == ConsoleMode.LINEAR:
                    self.console.log(f"Progress: {100*queue_progress:.2f}%")

            # Make the console pretty upon exit
            if self.__console_mode == ConsoleMode.INTERACTIVE:
                progress.update(task1, completed=1.0)

        # Fetch all remote samples at the controller#
        with self.console.status("Collecting remote results ...", spinner="dots") if self.__console_mode == ConsoleMode.INTERACTIVE else nullcontext():  # type: ignore

            evaluation_results: list[EvaluationResult] = ray.get(collector.fetch_results.remote())  # type: ignore

        # Measure elapsed time
        stop_time = perf_counter()
        performance_time = stop_time - start_time

        # Compute the final result
        simulation_result = MonteCarloResult(
            _dimensions, self.__evaluators, evaluation_results, 0.0
        )

        # Print results
        if self.__console_mode == ConsoleMode.LINEAR:
            self.console.print("")
            simulation_result.print(self.console)

        # Print finish notifier
        if self.__console_mode != ConsoleMode.SILENT:
            self.console.print()
            self.console.log(f"Simulation finished after {performance_time:.2f} seconds")

        return simulation_result

    @property
    def investigated_object(self) -> object:
        """The object to be investigated during the simulation runtime."""

        return self.__investigated_object

    def new_dimension(
        self, dimension: str, sample_points: list[object], *args: object, **kwargs
    ) -> GridDimension:
        """Add a dimension to the simulation grid.

        Must be a property of the investigated object.

        Args:

            dimension:
                String representation of dimension location relative to the investigated object.

            sample_points:
                list points at which the dimension will be sampled into a grid.
                The type of points must be identical to the grid arguments / type.

            args:
                References to the object the dimension belongs to.
                Resolved to the investigated object by default,
                but may be an attribute or sub-attribute of the investigated object.

            kwargs:
                Additional initialization arguments passed to :class:`GridDimension<hermespy.core.pymonte.grid.GridDimension>`.

        Returns: The newly created dimension object.
        """

        considered_objects = (self.__investigated_object,) if len(args) < 1 else args
        grid_dimension = GridDimension(considered_objects, dimension, sample_points, **kwargs)
        self.add_dimension(grid_dimension)

        return grid_dimension

    def add_dimension(self, dimension: GridDimension) -> None:
        """Add a new dimension to the simulation grid.

        Args:
            dimension: Dimension to be added.

        Raises:
            ValueError: If the `dimension` already exists within the grid.
        """

        if dimension in self.__dimensions:
            raise ValueError("Dimension instance already registered within the grid")

        self.__dimensions.append(dimension)

    def remove_dimension(self, dimension: GridDimension) -> None:
        """Remove an existing dimension from the simulation grid.

        Args:
            dimension: The dimension to be removed.

        Raises:
            ValueError: If the `dimension` does not exist.
        """

        if dimension not in self.__dimensions:
            raise ValueError("Dimension not registered within the current simulation grid")

        self.__dimensions.remove(dimension)

    @property
    def dimensions(self) -> list[GridDimension]:
        """Simulation grid dimensions which make up the grid."""

        return self.__dimensions.copy()

    def add_evaluator(self, evaluator: Evaluator) -> None:
        """Add new evaluator to the Monte Carlo simulation.

        Args:
            evaluator: The evaluator to be added.
        """

        self.__evaluators.append(evaluator)

    @property
    def evaluators(self) -> list[Evaluator]:
        """Evaluators used to process the investigated object sample state."""

        return self.__evaluators.copy()

    @property
    def num_samples(self) -> int:
        """Number of samples per simulation grid section.

        This represents the maximum number of samples a parameter combination will be sampled.
        The final number of collected samples may be smaller if the configured evaluators report confidence in their results before the maximum number of samples is reached.

        Raises:
            ValueError: If number of samples is smaller than one.
        """

        return self.__num_samples

    @num_samples.setter
    def num_samples(self, value: int) -> None:
        if value < 1:
            raise ValueError("Number of samples must be greater than zero")

        self.__num_samples = value

    @property
    def num_actors(self) -> int:
        """Number of dedicated actors spawned during simulation runs.

        Raises:
            ValueError: If the number of actors is smaller than zero.
        """

        # Return the number of available CPU cores as default value
        if self.__num_actors is not None:
            return self.__num_actors

        # Otherwise, the number of actors depends on the number of available CPUs and
        # the cpu requirements per actor
        return max(1, int(ray.available_resources().get("CPU", 1) / self.cpus_per_actor))

    @num_actors.setter
    def num_actors(self, value: int | None) -> None:
        """Set number of dedicated actors spawned during simulation runs."""

        if value is None:
            self.__num_actors = None

        elif value < 1:
            raise ValueError("Number of actors must be greater or equal to one")

        else:
            self.__num_actors = value

    @property
    def console(self) -> Console:
        """Console the Simulation writes to."""

        return self.__console

    @console.setter
    def console(self, value: Console) -> None:
        self.__console = value

    @property
    def batch_size(self) -> int:
        """Number of discrete samples generated by

        Raises:
            ValueError: If the block size is smaller than one.
        """

        if self.__batch_size is not None:
            return self.__batch_size

        size = 1
        for dimension in self.__dimensions:
            size *= dimension.num_sample_points

        return size

    @batch_size.setter
    def batch_size(self, value: int | None) -> None:
        if value is not None and value < 1:
            raise ValueError("Section block size must be greater or equal to one")

        self.__batch_size = value

    @property
    def cpus_per_actor(self) -> int:
        """Number of CPU cores reserved for each actor.

        Raises:
            ValueError: If the number of cores is smaller than one
        """

        return self.__cpus_per_actor

    @cpus_per_actor.setter
    def cpus_per_actor(self, num: int) -> None:
        if num < 1:
            raise ValueError("Number if CPU cores per actor must be greater or equal to one")

        self.__cpus_per_actor = num

    @property
    def console_mode(self) -> ConsoleMode:
        """Console output behaviour during simulation runtime."""

        return self.__console_mode

    @console_mode.setter
    def console_mode(self, value: ConsoleMode | str) -> None:
        # Convert string arguments to iterable
        if isinstance(value, str):
            value = ConsoleMode[value]

        self.__console_mode = value

    @staticmethod
    def _py_modules() -> list[str]:
        """list of python modules required by remote workers.

        In order to deploy Ray to computing clusters, dependencies listed here
        will be installed on remote nodes.

        Returns: list of module names.
        """

        return [path.join(path.dirname(path.realpath(__file__)), "..")]

    @staticmethod
    def _pip_packages() -> list[str]:
        """list of python packages required by remote workers.

        In order to deploy Ray to computing clusters, dependencies listed here
        will be installed on remote nodes.

        Returns: list of package names.
        """

        return ["ray", "numpy", "scipy", "matplotlib", "rich"]
