# -*- coding: utf-8 -*-

from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Generic, Sequence, TypeVar

import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np
from matplotlib.axis import Axis
from matplotlib.lines import Line2D
from mpl_toolkits.mplot3d import Axes3D  # type: ignore
from rich.console import Console
from rich.table import Table

from ..logarithmic import lin2db, ValueType
from ..visualize import PlotVisualization, Visualizable, VT
from .artifact import Artifact
from .grid import GridDimension

__author__ = "Jan Adler"
__copyright__ = "Copyright 2025, Barkhausen Institut gGmbH"
__credits__ = ["Jan Adler"]
__license__ = "AGPLv3"
__version__ = "1.5.0"
__maintainer__ = "Jan Adler"
__email__ = "jan.adler@barkhauseninstitut.org"
__status__ = "Prototype"


ET = TypeVar("ET", bound="Evaluation")
"""Type of Monte Carlo evaluation."""


EV = TypeVar("EV", bound="Evaluator")
"""Type of Monte Carlo evalutor."""


class Evaluation(Generic[VT], Visualizable[VT]):
    """Evaluation of a single simulation sample.

    Evaluations are generated by :class:`Evaluators <Evaluator>`
    during :meth:`Evaluator.evaluate`.
    """

    @abstractmethod
    def artifact(self) -> Artifact:
        """Generate an artifact from this evaluation.

        Returns: The evaluation artifact.
        """
        ...  # pragma: no cover


class EvaluationTemplate(Generic[ET, VT], Evaluation[VT], ABC):
    """Template class for simple evaluations containing a single object."""

    __evaluation: ET  # Evaluation

    def __init__(self, evaluation: ET) -> None:
        """
        Args:

            evaluation: The represented evaluation.
        """

        # Initialize base class
        super().__init__()

        # Initialize class attributes
        self.__evaluation = evaluation

    @property
    def evaluation(self) -> ET:
        """The represented evaluation."""

        return self.__evaluation



class EvaluationResult(Visualizable[PlotVisualization], ABC):
    """Result of an evaluation routine iterating over a parameter grid.

    Evaluation results are generated by :class:`Evaluator Instances <.Evaluator>` as a final
    step within the evaluation routine.
    """

    __grid: Sequence[GridDimension]
    __evaluator: Evaluator | None

    def __init__(self, grid: Sequence[GridDimension], evaluator: Evaluator | None = None) -> None:
        """
        Args:

            grid:
                Parameter grid over which the simulation generating this result iterated.

            evaluator:
                Evaluator that generated this result.
                If not specified, the result is considered to be generated by an unknown evaluator.
        """

        # Initialize base class
        Visualizable.__init__(self)

        # Initialize class attributes
        self.__grid = grid
        self.__evaluator = evaluator

    @property
    def grid(self) -> Sequence[GridDimension]:
        """Paramter grid over which the simulation iterated."""

        return self.__grid

    @property
    def evaluator(self) -> Evaluator | None:
        """Evaluator that generated this result."""

        return self.__evaluator

    @abstractmethod
    def add_artifact(
        self,
        coordinates: tuple[int, ...],
        artifact: Artifact,
        compute_confidence: bool = True,
    ) -> bool:
        """Add an artifact to this evaluation result.

        Args:
            coordinates: Coordinates of the grid section to which the artifact belongs.
            artifact: Artifact to be added.
            compute_confidence: Whether to compute the confidence level of the evaluation result for the given section coordinates.

        Returns:
            Can the result be trusted?
            Always :py:obj:`False` if `compute_confidence` is set to :py:obj:`False`.
        """
        ...  # pragma: no cover

    @abstractmethod
    def runtime_estimates(self) -> None | np.ndarray:
        ...

    def __result_to_str(self, value: object) -> str:
        """Convert a single result value to its string representation.

        Args:

            value: The value to be converted.

        Returns: The values string representation.
        """

        if isinstance(value, (int, float, complex)):
            if self.evaluator.tick_format == ValueType.DB or self.evaluator.plot_scale == "log":
                return f"{lin2db(value):.2g} dB"
            return f"{value:.2g}"
        return str(value)

    def print(self, console: Console | None = None) -> None:
        """Print a readable version of this evaluation result.

        Args:
            console:
                Rich console to print in.
                If not provided, a new one will be generated.
        """

        _console = Console() if console is None else console

        # Render results into a rich table
        table = Table()

        for dimension in self.grid:
            table.add_column(dimension.title, style="cyan")
        table.add_column("?" if self.evaluator is None else self.evaluator.abbreviation)

        for grid_section in np.ndindex(*[d.num_sample_points for d in self.grid]):
            columns = [g.sample_points[s].title for s, g in zip(grid_section, self.grid)]
            columns.append(self.to_str(grid_section))

        _console.print(table)

    def to_str(self, grid_coordinates: Sequence[int]) -> str:
        """Convert the evaluation result at the specified grid coordinates to a string representation.

        Args:
            grid_coordinates: Coordinates of the grid section to be converted.

        Returns: The string representation of the evaluation result.
        """

        results = self.to_array()
        result = results[tuple(grid_coordinates)]

        if isinstance(result, np.ndarray):
            result_str = ""
            for value in result.flat:
                result_str += self.__result_to_str(value) + ", "
            result_str = result_str[:-2]
        else:
            result_str = self.__result_to_str(result)

        return result_str

    @abstractmethod
    def to_array(self) -> np.ndarray:
        """Convert the evaluation result raw data to an array representation.

        Used to store the results in arbitrary binary file formats after simulation execution.

        Returns: The array result representation.
        """
        ...  # pragma: no cover

    @staticmethod
    def _configure_axis(axis: Axis, tick_format: ValueType = ValueType.LIN) -> None:
        """Subroutine to configure an axis.

        Args:
            axis: The matplotlib axis to be configured.
            tick_format: The tick format.
        """

        if tick_format is ValueType.DB:
            axis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f"{lin2db(x):.2g}dB"))

    def _prepare_surface_visualization(self, ax: Axes3D) -> list[Line2D]:
        # Configure axes labels and scales
        ax.set(
            xlabel=self.grid[0].title, ylabel=self.grid[1].title, zlabel=self.evaluator.abbreviation
        )

        EvaluationResult._configure_axis(ax.xaxis, self.grid[0].tick_format)
        EvaluationResult._configure_axis(ax.yaxis, self.grid[1].tick_format)
        if self.evaluator is not None:
            EvaluationResult._configure_axis(ax.zaxis, self.evaluator.tick_format)

        # x_points = np.asarray([s.value for s in self.grid[0].sample_points])
        # y_points = np.asarray([s.value for s in self.grid[1].sample_points])
        # y, x = np.meshgrid(y_points, x_points)

        # ax.plot_surface(x.astype(float), y.astype(float), np.zeros_like(y, dtype=np.float64))
        return []  # 3D plotting returns a poly3d collection that is not supported

    def _update_surface_visualization(
        self, data: np.ndarray, visualization: PlotVisualization
    ) -> None:
        """Plot two-dimensional simulation results into a three-dimensional axes system."""

        x_points = np.asarray([s.value for s in self.grid[0].sample_points])
        y_points = np.asarray([s.value for s in self.grid[1].sample_points])
        y, x = np.meshgrid(y_points, x_points)

        visualization.axes[0, 0].plot_surface(x.astype(float), y.astype(float), data)

    def _prepare_multidim_visualization(self, ax: plt.Axes) -> list[Line2D]:
        # Abort if the grid contains no data
        if len(self.grid) < 1:
            return []

        # For now, the x-dimension will always be the first simulation iteration dimension
        x_dimension = 0
        x_points = np.asarray([s.value for s in self.grid[x_dimension].sample_points])

        # Configure axes labels
        ax.set_xlabel(self.grid[x_dimension].title)
        ax.set_ylabel(self.evaluator.abbreviation)

        # Configure axes scales
        if self.evaluator is not None:
            ax.set_yscale(self.evaluator.plot_scale)
        ax.set_xscale(self.grid[x_dimension].plot_scale)

        # Configure axes ticks
        EvaluationResult._configure_axis(ax.xaxis, self.grid[x_dimension].tick_format)
        if self.evaluator is not None:
            EvaluationResult._configure_axis(ax.yaxis, self.evaluator.tick_format)

        subgrid = list(self.grid).copy()
        del subgrid[x_dimension]

        lines: list[Line2D] = []
        section_magnitudes = tuple(s.num_sample_points for s in subgrid)
        for section_indices in np.ndindex(section_magnitudes):
            # Generate the graph line label
            line_label = ""
            for i, v in enumerate(section_indices):
                line_label += f"{self.grid[i+1].title} = {self.grid[i+1].sample_points[v].title}, "
            line_label = line_label[:-2]

            if len(line_label) < 1:
                line_label = None

            # Plot the graph line
            lines.extend(
                ax.plot(x_points, np.zeros_like(x_points, dtype=np.float64), label=line_label)
            )

        # Add a legend to the plot
        # Only required if more than one line is plotted
        if len(self.grid) > 1:
            ax.legend()

        return lines

    def _update_multidim_visualization(
        self, scalar_data: np.ndarray, visualization: PlotVisualization
    ) -> None:
        """Plot multidimensional simulation results into a two-dimensional axes system.

        Args:
            scalar_data: Scalar data to be plotted.
            visualization: Visualization to plot into.
        """

        x_dimension = 0
        subgrid = list(self.grid).copy()
        del subgrid[x_dimension]

        section_magnitudes = tuple(s.num_sample_points for s in subgrid)
        line: Line2D
        for section_indices, line in zip(np.ndindex(section_magnitudes), visualization.lines[0, 0]):
            # Generate the graph line label
            line_label = ""
            for i, v in enumerate(section_indices):
                line_label += f"{self.grid[i+1].title} = {self.grid[i+1].sample_points[v].title}, "
            line_label = line_label[:-2]

            if len(line_label) < 1:
                line_label = None

            # Select the graph line scalars
            line_scalars = scalar_data[(..., *section_indices)]  # type: ignore

            # Update the respective graph line
            line.set_ydata(line_scalars)

        visualization.axes[0, 0].relim()
        visualization.axes[0, 0].autoscale_view()

    @staticmethod
    def _plot_empty(visualization: PlotVisualization) -> None:
        """Plot an empty notice into the provided axes.

        Args:
            plot: Plot to visualize in.
        """

        ax: plt.Axes
        for ax in visualization.axes.flat:
            ax.text(0.5, 0.5, "NO DATA AVAILABLE", horizontalalignment="center")

class Evaluator(ABC):
    """Evaluation routine for investigated object states, extracting performance indicators of interest.

    Evaluators represent the process of extracting arbitrary performance indicator samples :math:`X_m` in the form of
    :class:`.Artifact` instances from investigated object states.
    Once a :class:`.MonteCarloActor` has set its investigated object to a new random state,
    it calls the :meth:`evaluate<hermespy.core.monte_carlo.Evaluator.evaluate>` routines of all configured evaluators,
    collecting the resulting respective :class:`.Artifact` instances.

    For a given set of :class:`.Artifact` instances,
    evaluators are expected to report a :meth:`.confidence` which may result in a premature abortion of the
    sample collection routine for a single :class:`.GridSection`.
    By default, the routine suggested by :footcite:t:`2014:bayer` is applied:
    Considering a tolerance :math:`\\mathrm{TOL} \\in \\mathbb{R}_{++}` the confidence in the mean performance indicator

    .. math::

        \\bar{X}_M = \\frac{1}{M} \\sum_{m = 1}^{M} X_m

    is considered  sufficient if a threshold :math:`\\delta \\in \\mathbb{R}_{++}`, defined as

    .. math::

        \\mathrm{P}\\left(\\left\\| \\bar{X}_M - \\mathrm{E}\\left[ X \\right] \\right\\| > \\mathrm{TOL} \\right) \\leq \\delta

    has been crossed.
    The number of collected actually collected artifacts per :class:`.GridSection` :math:`M \\in [M_{\\mathrm{min}}, M_{\\mathrm{max}}]`
    is between a minimum number of required samples :math:`M_{\\mathrm{min}} \\in \\mathbb{R}_{+}` and an upper limit of
    :math:`M_{\\mathrm{max}} \\in \\mathbb{R}_{++}`.
    """

    tick_format: ValueType
    __confidence: float
    __tolerance: float
    __plot_scale: str  # Plot axis scaling

    def __init__(self) -> None:
        self.confidence = 1.0
        self.tolerance = 0.0
        self.plot_scale = "linear"
        self.tick_format = ValueType.LIN

    @abstractmethod
    def evaluate(self) -> Evaluation:
        """Evaluate the state of an investigated object.

        Implements the process of extracting an arbitrary performance indicator, represented by
        the returned :class:`.Artifact` :math:`X_m`.

        Returns: Artifact :math:`X_m` resulting from the evaluation.
        """
        ...  # pragma: no cover

    @property
    @abstractmethod
    def abbreviation(self) -> str:
        """Short string representation of this evaluator.

        Used as a label for console output and plot axes annotations.
        """
        ...  # pragma: no cover

    @property
    @abstractmethod
    def title(self) -> str:
        """Long string representation of this evaluator.

        Used as plot title.
        """
        ...  # pragma: no cover

    @property
    def confidence(self) -> float:
        """Confidence threshold required for premature simulation abortion.

        The confidence threshold :math:`\\delta \\in [0, 1]` is the upper bound to the
        confidence level

        .. math::

            \\mathrm{P}\\left(\\left\\| \\bar{X}_M - \\mathrm{E}\\left[ X \\right] \\right\\| > \\mathrm{TOL} \\right)

        at which the sample collection for a single :class:`.GridSection` may be prematurely aborted :footcite:p:`2014:bayer`.

        Raises:
            ValueError: If confidence is lower than zero or greater than one.
        """

        return self.__confidence

    @confidence.setter
    def confidence(self, value: float) -> None:
        if value < 0.0 or value > 1.0:
            raise ValueError("Confidence level must be in the interval between zero and one")

        self.__confidence = value

    @property
    def tolerance(self) -> float:
        """Tolerance level required for premature simulation abortion.

        The tolerance :math:`\\mathrm{TOL} \\in \\mathbb{R}_{++}` is the upper bound to the interval

        .. math::

           \\left\\| \\bar{X}_M - \\mathrm{E}\\left[ X \\right] \\right\\|

        by which the performance indicator estimation :math:`\\bar{X}_M` may diverge from the actual expected
        value :math:`\\mathrm{E}\\left[ X \\right]`.

        Returns: Non-negative tolerance :math:`\\mathrm{TOL}`.

        Raises:
            ValueError: If tolerance is negative.
        """

        return self.__tolerance

    @tolerance.setter
    def tolerance(self, value: float) -> None:
        if value < 0.0:
            raise ValueError("Tolerance must be greater or equal to zero")

        self.__tolerance = value

    def __str__(self) -> str:
        """String object representation.

        Returns: String representation.
        """

        return self.abbreviation

    @property
    def plot_scale(self) -> str:
        """Scale of the scalar evaluation plot.

        Refer to the `Matplotlib`_ documentation for a list of a accepted values.

        Returns: The scale identifier string.

        .. _Matplotlib: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_yscale.html
        """

        return self.__plot_scale

    @plot_scale.setter
    def plot_scale(self, value: str) -> None:
        self.__plot_scale = value

    @abstractmethod
    def initialize_result(self, grid: Sequence[GridDimension]) -> EvaluationResult:
        ...  # pragma: no cover

    @abstractmethod
    def generate_result(
        self, grid: Sequence[GridDimension], artifacts: np.ndarray
    ) -> EvaluationResult:
        """Generates an evaluation result from the artifacts collected over the whole simulation grid.

        Args:
            grid: The Simulation grid.
            artifacts: Numpy object array whose dimensions represent grid dimensions.

        Returns: The evaluation result.
        """
        ...  # pragma: no cover
