# -*- coding: utf-8 -*-

from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Generic, Sequence, TypeVar
from typing_extensions import override

import numpy as np
from matplotlib.axes import Axes
from matplotlib.axis import Axis
from matplotlib.figure import Figure
from matplotlib.lines import Line2D
import matplotlib.ticker as ticker
from rich.console import Console
from rich.table import Table

from ..logarithmic import lin2db, ValueType
from ..visualize import PlotVisualization, Visualizable, VAT, VT
from .artifact import Artifact, AT
from .grid import GridDimensionInfo

__author__ = "Jan Adler"
__copyright__ = "Copyright 2025, Barkhausen Institut gGmbH"
__credits__ = ["Jan Adler"]
__license__ = "AGPLv3"
__version__ = "1.5.0"
__maintainer__ = "Jan Adler"
__email__ = "jan.adler@barkhauseninstitut.org"
__status__ = "Prototype"


ET = TypeVar("ET", bound=object)
"""Type of Monte Carlo evaluation."""


EV = TypeVar("EV", bound="Evaluator")
"""Type of Monte Carlo evalutor."""


class Evaluation(Generic[VT], Visualizable[VT]):
    """Evaluation of a single simulation sample.

    Evaluations are generated by :class:`Evaluators <Evaluator>`
    during :meth:`Evaluator.evaluate`.
    """

    def __init__(self) -> None:
        # Initialize base class
        Visualizable.__init__(self)

    @abstractmethod
    def artifact(self) -> Artifact:
        """Generate an artifact from this evaluation.

        Returns: The evaluation artifact.
        """
        ...  # pragma: no cover


class EvaluationTemplate(Generic[ET, VT], Evaluation[VT], ABC):
    """Template class for simple evaluations containing a single object."""

    __evaluation: ET  # Evaluation

    def __init__(self, evaluation: ET) -> None:
        """
        Args:

            evaluation: The represented evaluation.
        """

        # Initialize base class
        super().__init__()

        # Initialize class attributes
        self.__evaluation = evaluation

    @property
    def evaluation(self) -> ET:
        """The represented evaluation."""

        return self.__evaluation


class EvaluationResult(Generic[AT], Visualizable[PlotVisualization], ABC):
    """Result of an evaluation routine iterating over a parameter grid.

    Evaluation results are generated by :class:`Evaluator Instances <.Evaluator>` as a final
    step within the evaluation routine.
    """

    __grid: Sequence[GridDimensionInfo]
    __evaluator: Evaluator | None
    __base_dimension_index: int

    def __init__(
        self,
        grid: Sequence[GridDimensionInfo],
        evaluator: Evaluator | None = None,
        base_dimension_index: int = 0,
    ) -> None:
        """
        Args:

            grid:
                Parameter grid over which the simulation generating this result iterated.

            evaluator:
                Evaluator that generated this result.
                If not specified, the result is considered to be generated by an unknown evaluator.
        """

        # Initialize base class
        Visualizable.__init__(self)

        # Initialize class attributes
        self.__grid = grid
        self.__evaluator = evaluator
        self.__base_dimension_index = base_dimension_index

    @property
    def grid(self) -> Sequence[GridDimensionInfo]:
        """Paramter grid over which the simulation iterated."""

        return self.__grid

    @property
    def evaluator(self) -> Evaluator | None:
        """Evaluator that generated this result."""

        return self.__evaluator

    @property
    def base_dimension_index(self) -> int:
        """Index of the base dimension used for plotting."""

        return self.__base_dimension_index

    @abstractmethod
    def add_artifact(
        self, coordinates: tuple[int, ...], artifact: AT, compute_confidence: bool = True
    ) -> bool:
        """Add an artifact to this evaluation result.

        Args:
            coordinates: Coordinates of the grid section to which the artifact belongs.
            artifact: Artifact to be added.
            compute_confidence: Whether to compute the confidence level of the evaluation result for the given section coordinates.

        Returns:
            Can the result be trusted?
            Always :py:obj:`False` if `compute_confidence` is set to :py:obj:`False`.
        """
        ...  # pragma: no cover

    @abstractmethod
    def runtime_estimates(self) -> None | np.ndarray:
        """Extract a runtime estimate for this evaluation result.

        Returns: A numpy array containing the runtime estimates for each grid section. If no estimates are available, :py:obj:`None` is returned.
        """
        ...  # pragma: no cover

    def __result_to_str(self, value: object) -> str:
        """Convert a single result value to its string representation.

        Args:

            value: The value to be converted.

        Returns: The values string representation.
        """

        if isinstance(value, (int, float, complex)):
            if self.evaluator.tick_format == ValueType.DB or self.evaluator.plot_scale == "log":
                return f"{lin2db(value):.2g} dB"
            return f"{value:.2g}"
        return str(value)

    def print(self, console: Console | None = None) -> None:
        """Print a readable version of this evaluation result.

        Args:
            console:
                Rich console to print in.
                If not provided, a new one will be generated.
        """

        _console = Console() if console is None else console

        # Render results into a rich table
        table = Table()

        for dimension in self.grid:
            table.add_column(dimension.title, style="cyan")
        table.add_column("?" if self.evaluator is None else self.evaluator.abbreviation)

        for grid_section in np.ndindex(*[d.num_sample_points for d in self.grid]):
            columns = [g.sample_points[s].title for s, g in zip(grid_section, self.grid)]
            columns.append(self.to_str(grid_section))

        _console.print(table)

    def to_str(self, grid_coordinates: Sequence[int]) -> str:
        """Convert the evaluation result at the specified grid coordinates to a string representation.

        Args:
            grid_coordinates: Coordinates of the grid section to be converted.

        Returns: The string representation of the evaluation result.
        """

        results = self.to_array()
        result = results[tuple(grid_coordinates)]

        if isinstance(result, np.ndarray):
            result_str = ""
            for value in result.flat:
                result_str += self.__result_to_str(value) + ", "
            result_str = result_str[:-2]
        else:
            result_str = self.__result_to_str(result)

        return result_str

    @abstractmethod
    def to_array(self) -> np.ndarray:
        """Convert the evaluation result raw data to an array representation.

        Used to store the results in arbitrary binary file formats after simulation execution.

        Returns: The array result representation.
        """
        ...  # pragma: no cover

    @override
    def _prepare_visualization(
        self, figure: Figure | None, axes: VAT, **kwargs
    ) -> PlotVisualization:

        lines = self._prepare_multidim_visualization(axes[0, 0])
        line_array = np.empty((axes.shape[0], axes.shape[1]), dtype=np.object_)
        line_array[0, 0] = lines

        return PlotVisualization(figure, axes, line_array)

    @override
    def _update_visualization(self, visualization: PlotVisualization, **kwargs) -> None:
        # If the grid contains no data, dont plot anything
        if len(self.grid) < 1:
            return self._plot_empty(visualization)

        # Shuffle grid and respective scalar results so that the selected base dimension is always the first entry
        grid = list(self.grid).copy()
        grid.insert(0, grid.pop(self.base_dimension_index))
        scalars = np.moveaxis(self.to_array(), self.base_dimension_index, 0)

        self._update_multidim_visualization(scalars, visualization)

    @staticmethod
    def _configure_axis(axis: Axis, tick_format: ValueType = ValueType.LIN) -> None:
        """Subroutine to configure an axis.

        Args:
            axis: The matplotlib axis to be configured.
            tick_format: The tick format.
        """

        if tick_format is ValueType.DB:
            axis.set_major_formatter(ticker.FuncFormatter(lambda x, _: f"{lin2db(x):.2g}dB"))

    def _prepare_multidim_visualization(self, ax: Axes) -> list[Line2D]:
        # Abort if the grid contains no data
        if len(self.grid) < 1:
            return []

        # For now, the x-dimension will always be the first simulation iteration dimension
        x_dimension = 0
        x_points = np.asarray([s.value for s in self.grid[x_dimension].sample_points])

        # Configure axes labels
        ax.set_xlabel(self.grid[x_dimension].title)
        ax.set_ylabel(self.evaluator.abbreviation)

        # Configure axes scales
        if self.evaluator is not None:
            ax.set_yscale(self.evaluator.plot_scale)
        ax.set_xscale(self.grid[x_dimension].plot_scale)

        # Configure axes ticks
        self._configure_axis(ax.xaxis, self.grid[x_dimension].tick_format)
        if self.evaluator is not None:
            self._configure_axis(ax.yaxis, self.evaluator.tick_format)

        subgrid = list(self.grid).copy()
        del subgrid[x_dimension]

        lines: list[Line2D] = []
        section_magnitudes = tuple(s.num_sample_points for s in subgrid)
        for section_indices in np.ndindex(section_magnitudes):
            # Generate the graph line label
            line_label = ""
            for i, v in enumerate(section_indices):
                line_label += f"{self.grid[i+1].title} = {self.grid[i+1].sample_points[v].title}, "
            line_label = line_label[:-2]

            if len(line_label) < 1:
                line_label = None

            # Plot the graph line
            lines.extend(
                ax.plot(x_points, np.zeros_like(x_points, dtype=np.float64), label=line_label)
            )

        # Add a legend to the plot
        # Only required if more than one line is plotted
        if len(self.grid) > 1:
            ax.legend()

        return lines

    def _update_multidim_visualization(
        self, scalar_data: np.ndarray, visualization: PlotVisualization
    ) -> None:
        """Plot multidimensional simulation results into a two-dimensional axes system.

        Args:
            scalar_data: Scalar data to be plotted.
            visualization: Visualization to plot into.
        """

        x_dimension = 0
        subgrid = list(self.grid).copy()
        del subgrid[x_dimension]

        section_magnitudes = tuple(s.num_sample_points for s in subgrid)
        line: Line2D
        for section_indices, line in zip(np.ndindex(section_magnitudes), visualization.lines[0, 0]):
            # Generate the graph line label
            line_label = ""
            for i, v in enumerate(section_indices):
                line_label += f"{self.grid[i+1].title} = {self.grid[i+1].sample_points[v].title}, "
            line_label = line_label[:-2]

            if len(line_label) < 1:
                line_label = None

            # Select the graph line scalars
            line_scalars = scalar_data[(..., *section_indices)]  # type: ignore

            # Update the respective graph line
            line.set_ydata(line_scalars)

        visualization.axes[0, 0].relim()
        visualization.axes[0, 0].autoscale_view()

    @staticmethod
    def _plot_empty(visualization: PlotVisualization) -> None:
        """Plot an empty notice into the provided axes.

        Args:
            plot: Plot to visualize in.
        """

        ax: Axes
        for ax in visualization.axes.flat:
            ax.text(0.5, 0.5, "NO DATA AVAILABLE", horizontalalignment="center")


class Evaluator(ABC):
    """Evaluation routine for investigated object states, extracting performance indicators of interest.

    Evaluators represent the process of extracting arbitrary performance indicator samples :math:`X_m` in the form of
    :class:`.Artifact` instances from investigated object states.
    """

    tick_format: ValueType
    __plot_scale: str  # Plot axis scaling

    def __init__(self, plot_scale: str = "linear", tick_format: ValueType = ValueType.LIN) -> None:
        self.plot_scale = plot_scale
        self.tick_format = tick_format

    @abstractmethod
    def evaluate(self) -> Evaluation:
        """Evaluate the state of an investigated object.

        Implements the process of extracting an arbitrary performance indicator, represented by
        the returned :class:`.Artifact` :math:`X_m`.

        Returns: Artifact :math:`X_m` resulting from the evaluation.
        """
        ...  # pragma: no cover

    @property
    @abstractmethod
    def abbreviation(self) -> str:
        """Short string representation of this evaluator.

        Used as a label for console output and plot axes annotations.
        """
        ...  # pragma: no cover

    @property
    @abstractmethod
    def title(self) -> str:
        """Long string representation of this evaluator.

        Used as plot title.
        """
        ...  # pragma: no cover

    def __str__(self) -> str:
        """String object representation.

        Returns: String representation.
        """

        return self.abbreviation

    @property
    def plot_scale(self) -> str:
        """Scale of the scalar evaluation plot.

        Refer to the `Matplotlib`_ documentation for a list of a accepted values.

        Returns: The scale identifier string.

        .. _Matplotlib: https://matplotlib.org/stable/api/_as_gen/matplotlib.axes.Axes.set_yscale.html
        """

        return self.__plot_scale

    @plot_scale.setter
    def plot_scale(self, value: str) -> None:
        self.__plot_scale = value

    @abstractmethod
    def initialize_result(self, grid: Sequence[GridDimensionInfo]) -> EvaluationResult:
        """Initialize the respective result object for this evaluator.

        Args:
            grid: The parameter grid over which the simulation iterates.

        Returns: The initialized evaluation result.
        """
        ...  # pragma: no cover
